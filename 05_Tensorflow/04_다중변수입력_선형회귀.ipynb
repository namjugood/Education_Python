{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tkinter import *\n",
    "from tkinter import ttk\n",
    "import tkinter.scrolledtext as tkst\n",
    "from matplotlib.figure import Figure\n",
    "from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "marathon_2015_2017 = pd.read_csv(\"./marathon_2015_2017.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 읽어온 데이터에서 성별, 나이, 페이스, 10K, 20K, 30K, 최종기록을 필터링\n",
    "# 최종기록으로 정렬 - record 변수에 저장\n",
    "record = pd.DataFrame(marathon_2015_2017, columns= ['M/F', 'Age', 'Pace', \\\n",
    "           '10K', '20K', '30K', 'Official Time']).sort_values(by=['Official Time'])\n",
    "\n",
    "# 성별의 데이터릴 'M', 'F'에서 0과 1로 변환\n",
    "record['M/F'] = record['M/F'].map({'M':1, 'F':0})\n",
    "\n",
    "# 데이터 프레임을 리스트로 변환\n",
    "record_list = record.values.tolist()\n",
    "\n",
    "# gender_list 생성\n",
    "gender_list = ['Femail','Mail']\n",
    "\n",
    "# 버튼이 클릭되면 화면에 표시될 차트를 생성하여 구색을 갖추고,\n",
    "# gn변수에 등록해두어, 차트 표시명령 시 세트명령을 사용하도록 준비합니다\n",
    "grad_fig = Figure( figsize=(10,4), dpi=100)\n",
    "grad_ax = grad_fig.add_subplot(111)\n",
    "grad_ax.set_xlim(15,88)\n",
    "grad_ax.set_ylim(0,1300)\n",
    "grad_ax.set_xlabel(\"Age : Age on race day\")\n",
    "grad_ax.set_ylabel(\"Pace : Runner's overall minute per mile pace\")\n",
    "\n",
    "g_xdata, g_ydata = [], []\n",
    "gn, =grad_ax.plot([], [], 'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram():\n",
    "    # t_gCbbox.get()의 값은 선택된 'Male' 또는 'Female'\n",
    "    gender = t_gCbbox.get()\n",
    "    # gender_list.index(gender) 선택된 값을 gender_list의 인덱스를 이용해서 0 또는 1로 변환\n",
    "    t_g = int(gender_list.index(gender))\n",
    "    \n",
    "    # 나머지 스핀박스 값 추출\n",
    "    t_a = int(t_aSpbox.get())  # 나이\n",
    "    t_p = int(t_pSpbox.get())  # 페이스\n",
    "    \n",
    "    # 차트에 표시할 성별에 따른 점 색깔 설정\n",
    "    if(t_g):\n",
    "        gender_color = 'b'\n",
    "    else:\n",
    "        gender_color = 'r'\n",
    "    \n",
    "    # 선택된 성별에 따라 필터링\n",
    "    gender_record = record[ record['M/F'] == t_g]\n",
    "    # 선택된 나이에 따라서 필터링\n",
    "    gender_age_record = gender_record[ gender_record.Age == t_a ]\n",
    "    # 필터링된 데이터들을 모두 리스트로 변환\n",
    "    gender_age_record_list = gender_age_record.values.tolist()\n",
    "    \n",
    "    # 성별에 따른 색깔의 점으로 같은 연령별(x축) 페이스들(y축)을 표시\n",
    "    grad_ax.plot(gender_record.Age, gender_record.Pace, '.', \\\n",
    "                                         color = gender_color, alpha=0.3)\n",
    "    # 선택된 연령과 페이스로 노란색 다이아몬드 표시\n",
    "    grad_ax.plot(t_a, t_p, 'yd')\n",
    "    # 나이로 필터링 된 데이터에서 페이스에 대한 기술 통계값 계산\n",
    "    stat = gender_age_record['Pace'].describe()\n",
    "    \n",
    "    # 차트 타이틀 제작\n",
    "    title = 'Gender : '+gender_list[t_g]+', Age :'+str(t_a)\n",
    "    grad_ax.set_title(title)\n",
    "    \n",
    "    # 오른쪽 상단 위치에 annotate로 인원, 사분위 값들을 차례로 표시\n",
    "    grad_ax.annotate(\"%10s %7i\"%('Count : ', stat[0]), (75, 1200), fontsize=10)\n",
    "    grad_ax.annotate(\"%10s %7.3f\"%('Mean : ', stat[1]), (75, 1150), fontsize=10)\n",
    "    grad_ax.annotate(\"%10s %7.3f\"%('25% : ', stat[3]), (75, 1100), fontsize=10)\n",
    "    grad_ax.annotate(\"%10s %7.3f\"%('75% : ', stat[5]), (75, 1050), fontsize=10)\n",
    "    grad_fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seconds_to_hhmmss(seconds):\n",
    "    hours = seconds // (60*60)\n",
    "    seconds %= (60*60)\n",
    "    minutes = seconds // 60\n",
    "    seconds %= 60\n",
    "    return \"%02i:%02i%02i\" % (hours, minutes, seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning():\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    # 성별 나이 페이스 추출\n",
    "    gender = t_gCbbox.get()\n",
    "    t_g = int(gender_list.index(gender))  \n",
    "    t_a = int(t_aSpbox.get())  # 대상 데이터의 순위\n",
    "    t_p = int(t_pSpbox.get())\n",
    "    \n",
    "    # train 횟수, learning rate 추출\n",
    "    t_t = int(t_tSpbox.get())  \n",
    "    t_r = float(t_rSpbox.get())  \n",
    "    \n",
    "    # 성별 나이 페이스를 feature로, 최종기록을 target으로 분리\n",
    "    x_train = [ r[0:3] for r in record_list ]\n",
    "    y_train = [ r[-1] for r in record_list ]\n",
    "    \n",
    "    # Sequential model 생성\n",
    "    model = tf.keras.models.Sequential()\n",
    "    # 입력 값 종류 3개, 출력 1개\n",
    "    model.add(tf.keras.layers.Dense(1, input_shape=(3,)))\n",
    "    # 최적화 도구로 sgd\n",
    "    sgd = tf.keras.optimizers.SGD(lr=t_r)\n",
    "    # 평가도구 mse로 설정하고, 위 사항으로 모델을 완성\n",
    "    model.compile(loss='mse', optimizer=sgd)\n",
    "    # 학습 전에 컴파일된 모델을 요약하여 출력\n",
    "    model.summary()\n",
    "    # 학습\n",
    "    history = model.fit(np.array(x_train), np.array(y_train), epochs=t_t)\n",
    "    \n",
    "    # 예측값 및 중간 코스트값 출력을 위한 제목들\n",
    "    log_ScrolledText.insert(END, '\\nGender:'+gender_list[t_g]+'\\, Age:'+str(t_a)+\\\n",
    "                           ', Pace:'+str(t_p)+'\\n', 'TITLE')\n",
    "    log_ScrolledText.insert(END, '\\n\\nCost Decent\\n\\n', 'HEADER')\n",
    "    log_ScrolledText.insert(END, '%20s %20s' % ('Step', 'Cost')+'\\n\\n')\n",
    "    \n",
    "    # 100회 학습마다 코스트 값 출력\n",
    "    for step in range(t_t):\n",
    "        if step % 100 == 0:\n",
    "            cost_val = history.history['loss'][step]\n",
    "            log_ScrolledText.insert(END, \"%20i %20.5f\" % (step, cost_val)+'\\n')\n",
    "    \n",
    "    # predict 예측(선택한 성별 나이 페이스로 최종 기록 예측)\n",
    "    winner = [t_g, t_a, t_p]\n",
    "    time = model.predict(np.array([winner]))\n",
    "    # 시분초 형식의 예측기록과 초단위의 예측 기록을 한 번에 출력하기 위한 텍스트 구성\n",
    "    ml_time = seconds_to_hhmmss(time[0][0])+ '('+str(time[0][0])+')'\n",
    "    # 예측값 출력을 위한 제목\n",
    "    log_ScrolledText.insert(END, \"%20s\" % ('\\n\\nThe Prediction Recoreds\\n\\n'), \\\n",
    "                            'HEADER')\n",
    "    # 출력 내용의 열 제목들\n",
    "    log_ScrolledText.insert(END, \"%10s %10s %10s %50s\" % ('Gender','Age','Pace',\\\n",
    "                                    'Record Prediction(Second) at 42.195km')+'\\n\\n')\n",
    "    # 예측값 출력\n",
    "    log_ScrolledText.insert(END, \"%10s %10s %10s %50s\" % (gender_list[t_g], str(t_a), \\\n",
    "                                                         str(t_p), ml_time)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 1)                 4         \n",
      "=================================================================\n",
      "Total params: 4\n",
      "Trainable params: 4\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2000\n",
      "2489/2489 [==============================] - 2s 784us/step - loss: 721464.7980\n",
      "Epoch 2/2000\n",
      "2489/2489 [==============================] - 1s 467us/step - loss: 8194.1596\n",
      "Epoch 3/2000\n",
      "2489/2489 [==============================] - 1s 455us/step - loss: 6334.3134\n",
      "Epoch 4/2000\n",
      "2489/2489 [==============================] - 1s 447us/step - loss: 1468.8626\n",
      "Epoch 5/2000\n",
      "2489/2489 [==============================] - 1s 449us/step - loss: 4394.3922\n",
      "Epoch 6/2000\n",
      "2489/2489 [==============================] - 1s 456us/step - loss: 1810.2945\n",
      "Epoch 7/2000\n",
      "2489/2489 [==============================] - 1s 456us/step - loss: 5251.5187\n",
      "Epoch 8/2000\n",
      "2489/2489 [==============================] - 1s 457us/step - loss: 7962.5033\n",
      "Epoch 9/2000\n",
      "2489/2489 [==============================] - 1s 460us/step - loss: 10679.3880\n",
      "Epoch 10/2000\n",
      "2489/2489 [==============================] - 1s 469us/step - loss: 2960.6946\n",
      "Epoch 11/2000\n",
      "2489/2489 [==============================] - 1s 453us/step - loss: 696.1353\n",
      "Epoch 12/2000\n",
      "2489/2489 [==============================] - 1s 459us/step - loss: 4490.5067\n",
      "Epoch 13/2000\n",
      "2489/2489 [==============================] - 1s 463us/step - loss: 9754.6336\n",
      "Epoch 14/2000\n",
      "2489/2489 [==============================] - 1s 462us/step - loss: 2005.4902\n",
      "Epoch 15/2000\n",
      "2489/2489 [==============================] - 1s 464us/step - loss: 534.7084\n",
      "Epoch 16/2000\n",
      "2489/2489 [==============================] - 1s 461us/step - loss: 1219.2567\n",
      "Epoch 17/2000\n",
      "2489/2489 [==============================] - 1s 453us/step - loss: 1549.4245\n",
      "Epoch 18/2000\n",
      "2489/2489 [==============================] - 1s 443us/step - loss: 2825.0042\n",
      "Epoch 19/2000\n",
      "2489/2489 [==============================] - 1s 448us/step - loss: 2176.5006\n",
      "Epoch 20/2000\n",
      "2489/2489 [==============================] - 1s 451us/step - loss: 393.3532\n",
      "Epoch 21/2000\n",
      "2489/2489 [==============================] - 1s 452us/step - loss: 1867.6757\n",
      "Epoch 22/2000\n",
      "2489/2489 [==============================] - 1s 443us/step - loss: 3637.2451\n",
      "Epoch 23/2000\n",
      "2489/2489 [==============================] - 1s 459us/step - loss: 1109.1340\n",
      "Epoch 24/2000\n",
      "2489/2489 [==============================] - 1s 446us/step - loss: 3595.8230\n",
      "Epoch 25/2000\n",
      "2489/2489 [==============================] - 1s 447us/step - loss: 5549.6556\n",
      "Epoch 26/2000\n",
      "2489/2489 [==============================] - 1s 444us/step - loss: 422.6713\n",
      "Epoch 27/2000\n",
      "2489/2489 [==============================] - 1s 445us/step - loss: 2704.3451\n",
      "Epoch 28/2000\n",
      "2489/2489 [==============================] - 1s 445us/step - loss: 2952.2062\n",
      "Epoch 29/2000\n",
      "2489/2489 [==============================] - 1s 446us/step - loss: 9953.0164\n",
      "Epoch 30/2000\n",
      "2489/2489 [==============================] - 1s 446us/step - loss: 1202.7418\n",
      "Epoch 31/2000\n",
      "2489/2489 [==============================] - 1s 448us/step - loss: 4585.1363\n",
      "Epoch 32/2000\n",
      "2489/2489 [==============================] - 1s 451us/step - loss: 621.0118\n",
      "Epoch 33/2000\n",
      "2489/2489 [==============================] - 1s 450us/step - loss: 7652.1026\n",
      "Epoch 34/2000\n",
      "2489/2489 [==============================] - 1s 441us/step - loss: 5660.5952\n",
      "Epoch 35/2000\n",
      "2489/2489 [==============================] - 1s 439us/step - loss: 1447.5741\n",
      "Epoch 36/2000\n",
      "2489/2489 [==============================] - 1s 446us/step - loss: 6333.3343\n",
      "Epoch 37/2000\n",
      "2489/2489 [==============================] - 1s 444us/step - loss: 568.5211\n",
      "Epoch 38/2000\n",
      "2489/2489 [==============================] - 1s 446us/step - loss: 4464.9890\n",
      "Epoch 39/2000\n",
      "2489/2489 [==============================] - 1s 443us/step - loss: 9113.5632\n",
      "Epoch 40/2000\n",
      "2489/2489 [==============================] - 1s 448us/step - loss: 1079.5533\n",
      "Epoch 41/2000\n",
      "2489/2489 [==============================] - 1s 447us/step - loss: 2071.0618\n",
      "Epoch 42/2000\n",
      "2489/2489 [==============================] - 1s 446us/step - loss: 11691.3881\n",
      "Epoch 43/2000\n",
      "2489/2489 [==============================] - 1s 448us/step - loss: 3946.3079\n",
      "Epoch 44/2000\n",
      "2489/2489 [==============================] - 1s 451us/step - loss: 3178.7817\n",
      "Epoch 45/2000\n",
      "2489/2489 [==============================] - 1s 448us/step - loss: 5136.0235\n",
      "Epoch 46/2000\n",
      "2489/2489 [==============================] - 1s 437us/step - loss: 853.0529\n",
      "Epoch 47/2000\n",
      "2489/2489 [==============================] - 1s 443us/step - loss: 97.0465\n",
      "Epoch 48/2000\n",
      "2489/2489 [==============================] - 1s 443us/step - loss: 3818.7297\n",
      "Epoch 49/2000\n",
      "2489/2489 [==============================] - 1s 452us/step - loss: 14284.7436\n",
      "Epoch 50/2000\n",
      "2489/2489 [==============================] - 1s 458us/step - loss: 3936.7421\n",
      "Epoch 51/2000\n",
      "2489/2489 [==============================] - 1s 444us/step - loss: 942.2325\n",
      "Epoch 52/2000\n",
      "2489/2489 [==============================] - 1s 454us/step - loss: 1968.3460\n",
      "Epoch 53/2000\n",
      "2489/2489 [==============================] - 1s 457us/step - loss: 2224.5582\n",
      "Epoch 54/2000\n",
      "2489/2489 [==============================] - 1s 440us/step - loss: 830.8691\n",
      "Epoch 55/2000\n",
      "2489/2489 [==============================] - 1s 443us/step - loss: 5431.9774\n",
      "Epoch 56/2000\n",
      "2489/2489 [==============================] - 1s 441us/step - loss: 2036.7227\n",
      "Epoch 57/2000\n",
      "2489/2489 [==============================] - 1s 454us/step - loss: 705.6026\n",
      "Epoch 58/2000\n",
      "2489/2489 [==============================] - 1s 451us/step - loss: 2521.4321\n",
      "Epoch 59/2000\n",
      "2489/2489 [==============================] - 1s 436us/step - loss: 1317.9610\n",
      "Epoch 60/2000\n",
      "2489/2489 [==============================] - 1s 437us/step - loss: 1669.7805\n",
      "Epoch 61/2000\n",
      "2489/2489 [==============================] - 1s 443us/step - loss: 8401.3046\n",
      "Epoch 62/2000\n",
      "2489/2489 [==============================] - 1s 447us/step - loss: 2804.0076\n",
      "Epoch 63/2000\n",
      "2489/2489 [==============================] - 1s 444us/step - loss: 3273.9190\n",
      "Epoch 64/2000\n",
      "2489/2489 [==============================] - 1s 447us/step - loss: 15717.0828\n",
      "Epoch 65/2000\n",
      "2489/2489 [==============================] - 1s 450us/step - loss: 4361.3027\n",
      "Epoch 66/2000\n",
      "2489/2489 [==============================] - 1s 443us/step - loss: 6634.7810\n",
      "Epoch 67/2000\n",
      "2489/2489 [==============================] - 1s 443us/step - loss: 4112.5090\n",
      "Epoch 68/2000\n",
      "2489/2489 [==============================] - 1s 444us/step - loss: 9199.9439\n",
      "Epoch 69/2000\n",
      "2489/2489 [==============================] - 1s 454us/step - loss: 2134.0514\n",
      "Epoch 70/2000\n",
      "2489/2489 [==============================] - 1s 443us/step - loss: 558.2845\n",
      "Epoch 71/2000\n",
      "2489/2489 [==============================] - 1s 442us/step - loss: 5838.7945\n",
      "Epoch 72/2000\n",
      "2489/2489 [==============================] - 1s 440us/step - loss: 11145.7521\n",
      "Epoch 73/2000\n",
      "2489/2489 [==============================] - 1s 445us/step - loss: 4465.5633\n",
      "Epoch 74/2000\n",
      "2489/2489 [==============================] - 1s 442us/step - loss: 1295.7421\n",
      "Epoch 75/2000\n",
      "2489/2489 [==============================] - 1s 437us/step - loss: 684.1062\n",
      "Epoch 76/2000\n",
      "2489/2489 [==============================] - 1s 452us/step - loss: 4503.1117\n",
      "Epoch 77/2000\n",
      "2489/2489 [==============================] - 1s 448us/step - loss: 3891.7668\n",
      "Epoch 78/2000\n",
      "2489/2489 [==============================] - 1s 444us/step - loss: 3265.2956\n",
      "Epoch 79/2000\n",
      "2489/2489 [==============================] - 1s 443us/step - loss: 810.2497\n",
      "Epoch 80/2000\n",
      "2489/2489 [==============================] - 1s 460us/step - loss: 5701.2750\n",
      "Epoch 81/2000\n",
      "2489/2489 [==============================] - 1s 447us/step - loss: 7515.7554\n",
      "Epoch 82/2000\n",
      "2489/2489 [==============================] - 1s 437us/step - loss: 982.8390\n",
      "Epoch 83/2000\n",
      "2489/2489 [==============================] - 1s 447us/step - loss: 2583.0970\n",
      "Epoch 84/2000\n",
      "2489/2489 [==============================] - 1s 446us/step - loss: 1061.4806\n",
      "Epoch 85/2000\n",
      "2489/2489 [==============================] - 1s 451us/step - loss: 1031.7083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/2000\n",
      "2489/2489 [==============================] - 1s 445us/step - loss: 6578.8377\n",
      "Epoch 87/2000\n",
      "2489/2489 [==============================] - 1s 452us/step - loss: 6258.1598\n",
      "Epoch 88/2000\n",
      "2489/2489 [==============================] - 1s 443us/step - loss: 2781.0156\n",
      "Epoch 89/2000\n",
      "2489/2489 [==============================] - 1s 443us/step - loss: 747.5891\n",
      "Epoch 90/2000\n",
      "2489/2489 [==============================] - 1s 434us/step - loss: 461.7140\n",
      "Epoch 91/2000\n",
      "2489/2489 [==============================] - 1s 443us/step - loss: 750.9097\n",
      "Epoch 92/2000\n",
      "2489/2489 [==============================] - 1s 443us/step - loss: 5762.5699\n",
      "Epoch 93/2000\n",
      "2489/2489 [==============================] - 1s 449us/step - loss: 1959.2158\n",
      "Epoch 94/2000\n",
      "2489/2489 [==============================] - 1s 439us/step - loss: 201.0196\n",
      "Epoch 95/2000\n",
      "2489/2489 [==============================] - 1s 440us/step - loss: 2901.8934\n",
      "Epoch 96/2000\n",
      "2489/2489 [==============================] - 1s 439us/step - loss: 9810.4093\n",
      "Epoch 97/2000\n",
      "2489/2489 [==============================] - 1s 441us/step - loss: 1411.2048\n",
      "Epoch 98/2000\n",
      "2489/2489 [==============================] - 1s 437us/step - loss: 1273.1455\n",
      "Epoch 99/2000\n",
      "2489/2489 [==============================] - 1s 444us/step - loss: 1059.2063\n",
      "Epoch 100/2000\n",
      "2489/2489 [==============================] - 1s 441us/step - loss: 672.7694\n",
      "Epoch 101/2000\n",
      "2489/2489 [==============================] - 1s 448us/step - loss: 13732.5080\n",
      "Epoch 102/2000\n",
      "2489/2489 [==============================] - 1s 465us/step - loss: 2351.1841\n",
      "Epoch 103/2000\n",
      "2489/2489 [==============================] - 1s 450us/step - loss: 1284.0548\n",
      "Epoch 104/2000\n",
      "2489/2489 [==============================] - 1s 442us/step - loss: 615.0324\n",
      "Epoch 105/2000\n",
      "2489/2489 [==============================] - 1s 469us/step - loss: 6446.0479\n",
      "Epoch 106/2000\n",
      "2489/2489 [==============================] - 1s 463us/step - loss: 362.7997\n",
      "Epoch 107/2000\n",
      "2489/2489 [==============================] - 1s 533us/step - loss: 5696.8578\n",
      "Epoch 108/2000\n",
      "2489/2489 [==============================] - 1s 488us/step - loss: 2272.2748\n",
      "Epoch 109/2000\n",
      "2489/2489 [==============================] - 1s 441us/step - loss: 9679.7580\n",
      "Epoch 110/2000\n",
      "2489/2489 [==============================] - 1s 444us/step - loss: 276.7537\n",
      "Epoch 111/2000\n",
      "2489/2489 [==============================] - 1s 437us/step - loss: 680.2448\n",
      "Epoch 112/2000\n",
      "2489/2489 [==============================] - 1s 451us/step - loss: 2712.2916\n",
      "Epoch 113/2000\n",
      "2489/2489 [==============================] - 1s 444us/step - loss: 316.0561\n",
      "Epoch 114/2000\n",
      "2489/2489 [==============================] - 1s 457us/step - loss: 3449.2514\n",
      "Epoch 115/2000\n",
      "2489/2489 [==============================] - 1s 442us/step - loss: 4753.0042\n",
      "Epoch 116/2000\n",
      "2489/2489 [==============================] - 1s 438us/step - loss: 13121.4944\n",
      "Epoch 117/2000\n",
      "2489/2489 [==============================] - 1s 446us/step - loss: 1977.0308\n",
      "Epoch 118/2000\n",
      "2489/2489 [==============================] - 1s 435us/step - loss: 803.8627\n",
      "Epoch 119/2000\n",
      "2489/2489 [==============================] - 1s 431us/step - loss: 2273.0942\n",
      "Epoch 120/2000\n",
      "2489/2489 [==============================] - 1s 456us/step - loss: 1294.3727\n",
      "Epoch 121/2000\n",
      "2489/2489 [==============================] - 1s 432us/step - loss: 807.6457\n",
      "Epoch 122/2000\n",
      "2489/2489 [==============================] - 1s 448us/step - loss: 4455.0769\n",
      "Epoch 123/2000\n",
      "2489/2489 [==============================] - 1s 436us/step - loss: 1171.6032\n",
      "Epoch 124/2000\n",
      "2489/2489 [==============================] - 1s 440us/step - loss: 6784.3367\n",
      "Epoch 125/2000\n",
      "2489/2489 [==============================] - 1s 450us/step - loss: 2812.0024\n",
      "Epoch 126/2000\n",
      "2489/2489 [==============================] - 1s 440us/step - loss: 14108.8735\n",
      "Epoch 127/2000\n",
      "2489/2489 [==============================] - 1s 433us/step - loss: 342.6590\n",
      "Epoch 128/2000\n",
      "2489/2489 [==============================] - 1s 447us/step - loss: 1045.4932\n",
      "Epoch 129/2000\n",
      "2489/2489 [==============================] - 1s 438us/step - loss: 2411.0638\n",
      "Epoch 130/2000\n",
      "2489/2489 [==============================] - 1s 436us/step - loss: 4644.9427\n",
      "Epoch 131/2000\n",
      "2489/2489 [==============================] - 1s 438us/step - loss: 8074.7632\n",
      "Epoch 132/2000\n",
      "2489/2489 [==============================] - 1s 437us/step - loss: 10507.8587\n",
      "Epoch 133/2000\n",
      "2489/2489 [==============================] - 1s 444us/step - loss: 5238.2751\n",
      "Epoch 134/2000\n",
      "2489/2489 [==============================] - 1s 431us/step - loss: 415.1691\n",
      "Epoch 135/2000\n",
      "2489/2489 [==============================] - 1s 442us/step - loss: 3596.9345\n",
      "Epoch 136/2000\n",
      "2489/2489 [==============================] - 1s 442us/step - loss: 5033.8783\n",
      "Epoch 137/2000\n",
      "2489/2489 [==============================] - 1s 441us/step - loss: 7877.7610\n",
      "Epoch 138/2000\n",
      "2489/2489 [==============================] - 1s 441us/step - loss: 150.0836\n",
      "Epoch 139/2000\n",
      "2489/2489 [==============================] - 1s 443us/step - loss: 8211.1774\n",
      "Epoch 140/2000\n",
      "2489/2489 [==============================] - 1s 443us/step - loss: 12604.3358\n",
      "Epoch 141/2000\n",
      "2489/2489 [==============================] - 1s 442us/step - loss: 3053.0090\n",
      "Epoch 142/2000\n",
      "2489/2489 [==============================] - 1s 445us/step - loss: 2369.5526\n",
      "Epoch 143/2000\n",
      "2489/2489 [==============================] - 1s 433us/step - loss: 5978.9522\n",
      "Epoch 144/2000\n",
      "2489/2489 [==============================] - 1s 440us/step - loss: 16338.0991\n",
      "Epoch 145/2000\n",
      "2489/2489 [==============================] - 1s 441us/step - loss: 3389.1853\n",
      "Epoch 146/2000\n",
      "2489/2489 [==============================] - 1s 453us/step - loss: 952.6899\n",
      "Epoch 147/2000\n",
      "2489/2489 [==============================] - 1s 435us/step - loss: 4589.9442\n",
      "Epoch 148/2000\n",
      "2489/2489 [==============================] - 1s 437us/step - loss: 3287.4878\n",
      "Epoch 149/2000\n",
      "2489/2489 [==============================] - 1s 440us/step - loss: 1631.9554\n",
      "Epoch 150/2000\n",
      "2489/2489 [==============================] - 1s 439us/step - loss: 4467.0422\n",
      "Epoch 151/2000\n",
      "2489/2489 [==============================] - 1s 434us/step - loss: 5995.3410\n",
      "Epoch 152/2000\n",
      "2489/2489 [==============================] - 1s 445us/step - loss: 4052.7003\n",
      "Epoch 153/2000\n",
      "2489/2489 [==============================] - 1s 433us/step - loss: 9618.1104\n",
      "Epoch 154/2000\n",
      "2489/2489 [==============================] - 1s 446us/step - loss: 2903.1575\n",
      "Epoch 155/2000\n",
      "2489/2489 [==============================] - 1s 444us/step - loss: 3802.8823\n",
      "Epoch 156/2000\n",
      "2489/2489 [==============================] - 1s 439us/step - loss: 2274.5676\n",
      "Epoch 157/2000\n",
      "2489/2489 [==============================] - 1s 461us/step - loss: 2812.2633\n",
      "Epoch 158/2000\n",
      "2489/2489 [==============================] - 1s 449us/step - loss: 1552.4215\n",
      "Epoch 159/2000\n",
      "2489/2489 [==============================] - 1s 449us/step - loss: 1889.3785\n",
      "Epoch 160/2000\n",
      "2489/2489 [==============================] - 1s 438us/step - loss: 16456.0118\n",
      "Epoch 161/2000\n",
      "2489/2489 [==============================] - 1s 439us/step - loss: 1622.1118\n",
      "Epoch 162/2000\n",
      "2489/2489 [==============================] - 1s 446us/step - loss: 28321.4358\n",
      "Epoch 163/2000\n",
      "2489/2489 [==============================] - 1s 444us/step - loss: 8856.4068\n",
      "Epoch 164/2000\n",
      "2489/2489 [==============================] - 1s 439us/step - loss: 873.2173\n",
      "Epoch 165/2000\n",
      "2489/2489 [==============================] - 1s 435us/step - loss: 22905.8372\n",
      "Epoch 166/2000\n",
      "2489/2489 [==============================] - 1s 435us/step - loss: 11803.0278\n",
      "Epoch 167/2000\n",
      "2489/2489 [==============================] - 1s 447us/step - loss: 10519.7780\n",
      "Epoch 168/2000\n",
      "2489/2489 [==============================] - 1s 450us/step - loss: 2409.5862\n",
      "Epoch 169/2000\n",
      "2489/2489 [==============================] - 1s 430us/step - loss: 5607.4283\n",
      "Epoch 170/2000\n",
      "2489/2489 [==============================] - 1s 463us/step - loss: 2098.0063\n",
      "Epoch 171/2000\n",
      "2489/2489 [==============================] - 1s 440us/step - loss: 1583.9077\n",
      "Epoch 172/2000\n",
      "2489/2489 [==============================] - 1s 435us/step - loss: 201.8285\n",
      "Epoch 173/2000\n",
      "2489/2489 [==============================] - 1s 440us/step - loss: 17668.9146\n",
      "Epoch 174/2000\n",
      "2489/2489 [==============================] - 1s 445us/step - loss: 100.1146\n",
      "Epoch 175/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2489/2489 [==============================] - 1s 441us/step - loss: 467.8478\n",
      "Epoch 176/2000\n",
      "2489/2489 [==============================] - 1s 439us/step - loss: 325.5735\n",
      "Epoch 177/2000\n",
      "2489/2489 [==============================] - 1s 438us/step - loss: 5097.5193\n",
      "Epoch 178/2000\n",
      "2489/2489 [==============================] - 1s 454us/step - loss: 3479.8054\n",
      "Epoch 179/2000\n",
      "2489/2489 [==============================] - 1s 447us/step - loss: 19103.5277\n",
      "Epoch 180/2000\n",
      "2489/2489 [==============================] - 1s 442us/step - loss: 7816.6550\n",
      "Epoch 181/2000\n",
      "2489/2489 [==============================] - 1s 444us/step - loss: 6652.4974\n",
      "Epoch 182/2000\n",
      "2489/2489 [==============================] - 1s 439us/step - loss: 203.4922\n",
      "Epoch 183/2000\n",
      "2489/2489 [==============================] - 1s 447us/step - loss: 4239.3466\n",
      "Epoch 184/2000\n",
      "2489/2489 [==============================] - 1s 443us/step - loss: 740.2949\n",
      "Epoch 185/2000\n",
      "2489/2489 [==============================] - 1s 437us/step - loss: 127.2416\n",
      "Epoch 186/2000\n",
      "2489/2489 [==============================] - 1s 455us/step - loss: 2200.3008\n",
      "Epoch 187/2000\n",
      "2489/2489 [==============================] - 1s 440us/step - loss: 3472.1416\n",
      "Epoch 188/2000\n",
      "2489/2489 [==============================] - 1s 436us/step - loss: 5068.8478\n",
      "Epoch 189/2000\n",
      "2489/2489 [==============================] - 1s 447us/step - loss: 13801.5364\n",
      "Epoch 190/2000\n",
      "2489/2489 [==============================] - 1s 446us/step - loss: 252.9763\n",
      "Epoch 191/2000\n",
      "2489/2489 [==============================] - 1s 450us/step - loss: 1854.5195\n",
      "Epoch 192/2000\n",
      "2489/2489 [==============================] - 1s 445us/step - loss: 1709.7499\n",
      "Epoch 193/2000\n",
      "2489/2489 [==============================] - 1s 452us/step - loss: 1614.7646\n",
      "Epoch 194/2000\n",
      "2489/2489 [==============================] - 1s 475us/step - loss: 3315.7609\n",
      "Epoch 195/2000\n",
      "2489/2489 [==============================] - 1s 481us/step - loss: 11340.1096\n",
      "Epoch 196/2000\n",
      "2489/2489 [==============================] - 1s 484us/step - loss: 1894.2226\n",
      "Epoch 197/2000\n",
      "2489/2489 [==============================] - 1s 480us/step - loss: 2443.5248\n",
      "Epoch 198/2000\n",
      "2489/2489 [==============================] - 1s 456us/step - loss: 13436.3315\n",
      "Epoch 199/2000\n",
      "2489/2489 [==============================] - 1s 465us/step - loss: 2430.9471\n",
      "Epoch 200/2000\n",
      "2489/2489 [==============================] - 1s 444us/step - loss: 2878.3885\n",
      "Epoch 201/2000\n",
      "2489/2489 [==============================] - 1s 433us/step - loss: 906.3172\n",
      "Epoch 202/2000\n",
      "2489/2489 [==============================] - 1s 473us/step - loss: 570.0865\n",
      "Epoch 203/2000\n",
      "2489/2489 [==============================] - 1s 470us/step - loss: 831.8548\n",
      "Epoch 204/2000\n",
      "2489/2489 [==============================] - 1s 453us/step - loss: 1876.1573\n",
      "Epoch 205/2000\n",
      "2489/2489 [==============================] - 1s 462us/step - loss: 3171.9580\n",
      "Epoch 206/2000\n",
      "2489/2489 [==============================] - 1s 464us/step - loss: 17551.0434\n",
      "Epoch 207/2000\n",
      "2489/2489 [==============================] - 1s 457us/step - loss: 1322.9884\n",
      "Epoch 208/2000\n",
      "2489/2489 [==============================] - 1s 467us/step - loss: 2787.4168\n",
      "Epoch 209/2000\n",
      "2489/2489 [==============================] - 1s 466us/step - loss: 84.7894\n",
      "Epoch 210/2000\n",
      "2489/2489 [==============================] - 1s 479us/step - loss: 3264.9332\n",
      "Epoch 211/2000\n",
      "2489/2489 [==============================] - 1s 467us/step - loss: 7316.4071\n",
      "Epoch 212/2000\n",
      "2489/2489 [==============================] - 1s 453us/step - loss: 6138.1601\n",
      "Epoch 213/2000\n",
      "2489/2489 [==============================] - 1s 455us/step - loss: 4692.1476\n",
      "Epoch 214/2000\n",
      "2489/2489 [==============================] - 1s 453us/step - loss: 1214.0453\n",
      "Epoch 215/2000\n",
      "2489/2489 [==============================] - 1s 449us/step - loss: 154.9270\n",
      "Epoch 216/2000\n",
      "2489/2489 [==============================] - 1s 453us/step - loss: 9459.5988\n",
      "Epoch 217/2000\n",
      "2489/2489 [==============================] - 1s 454us/step - loss: 3014.1655\n",
      "Epoch 218/2000\n",
      "2489/2489 [==============================] - 1s 467us/step - loss: 1486.6984\n",
      "Epoch 219/2000\n",
      "2489/2489 [==============================] - 1s 455us/step - loss: 1246.5287\n",
      "Epoch 220/2000\n",
      "2489/2489 [==============================] - 1s 469us/step - loss: 599.5886\n",
      "Epoch 221/2000\n",
      "2489/2489 [==============================] - 1s 467us/step - loss: 21725.5265\n",
      "Epoch 222/2000\n",
      "2489/2489 [==============================] - 1s 462us/step - loss: 8232.1613\n",
      "Epoch 223/2000\n",
      "2489/2489 [==============================] - 1s 469us/step - loss: 3502.1241\n",
      "Epoch 224/2000\n",
      "2489/2489 [==============================] - 1s 457us/step - loss: 1628.3461\n",
      "Epoch 225/2000\n",
      "2489/2489 [==============================] - 1s 458us/step - loss: 6606.3251\n",
      "Epoch 226/2000\n",
      "2489/2489 [==============================] - 1s 452us/step - loss: 1244.9716\n",
      "Epoch 227/2000\n",
      "2489/2489 [==============================] - 1s 464us/step - loss: 5611.6340\n",
      "Epoch 228/2000\n",
      "2489/2489 [==============================] - 1s 455us/step - loss: 108.2094\n",
      "Epoch 229/2000\n",
      "2489/2489 [==============================] - 1s 454us/step - loss: 670.7149\n",
      "Epoch 230/2000\n",
      "2489/2489 [==============================] - 1s 455us/step - loss: 10003.5187\n",
      "Epoch 231/2000\n",
      "2489/2489 [==============================] - 1s 446us/step - loss: 11316.9303\n",
      "Epoch 232/2000\n",
      "2489/2489 [==============================] - 1s 459us/step - loss: 2086.0688\n",
      "Epoch 233/2000\n",
      "2489/2489 [==============================] - 1s 456us/step - loss: 8752.2656\n",
      "Epoch 234/2000\n",
      "2489/2489 [==============================] - 1s 459us/step - loss: 7333.7825\n",
      "Epoch 235/2000\n",
      "2489/2489 [==============================] - 1s 458us/step - loss: 4942.2362\n",
      "Epoch 236/2000\n",
      "2489/2489 [==============================] - 1s 459us/step - loss: 3153.1374\n",
      "Epoch 237/2000\n",
      "2489/2489 [==============================] - 1s 454us/step - loss: 8068.4891\n",
      "Epoch 238/2000\n",
      "2489/2489 [==============================] - 1s 458us/step - loss: 15656.7312\n",
      "Epoch 239/2000\n",
      "2489/2489 [==============================] - 1s 449us/step - loss: 393.4526\n",
      "Epoch 240/2000\n",
      "2489/2489 [==============================] - 1s 449us/step - loss: 8584.4933\n",
      "Epoch 241/2000\n",
      "2489/2489 [==============================] - 1s 461us/step - loss: 8644.7949\n",
      "Epoch 242/2000\n",
      "2489/2489 [==============================] - 1s 464us/step - loss: 3547.5559\n",
      "Epoch 243/2000\n",
      "2489/2489 [==============================] - 1s 453us/step - loss: 7197.1988\n",
      "Epoch 244/2000\n",
      "2489/2489 [==============================] - 1s 453us/step - loss: 1362.8135\n",
      "Epoch 245/2000\n",
      "2489/2489 [==============================] - 1s 451us/step - loss: 7522.2333\n",
      "Epoch 246/2000\n",
      "2489/2489 [==============================] - 1s 456us/step - loss: 2912.6471\n",
      "Epoch 247/2000\n",
      "2489/2489 [==============================] - 1s 453us/step - loss: 2078.1210\n",
      "Epoch 248/2000\n",
      "2489/2489 [==============================] - 1s 452us/step - loss: 7811.4357\n",
      "Epoch 249/2000\n",
      "2489/2489 [==============================] - 1s 451us/step - loss: 6338.4830\n",
      "Epoch 250/2000\n",
      "2489/2489 [==============================] - 1s 462us/step - loss: 3219.9351\n",
      "Epoch 251/2000\n",
      "2489/2489 [==============================] - 1s 460us/step - loss: 10633.4847\n",
      "Epoch 252/2000\n",
      "2489/2489 [==============================] - 1s 451us/step - loss: 9612.1775\n",
      "Epoch 253/2000\n",
      "2489/2489 [==============================] - 1s 451us/step - loss: 1946.5911\n",
      "Epoch 254/2000\n",
      "2489/2489 [==============================] - 1s 451us/step - loss: 3238.9054\n",
      "Epoch 255/2000\n",
      "2489/2489 [==============================] - 1s 454us/step - loss: 10043.8712\n",
      "Epoch 256/2000\n",
      "2489/2489 [==============================] - 1s 454us/step - loss: 457.6262\n",
      "Epoch 257/2000\n",
      "2489/2489 [==============================] - 1s 459us/step - loss: 2558.0234\n",
      "Epoch 258/2000\n",
      "2489/2489 [==============================] - 1s 455us/step - loss: 815.2522\n",
      "Epoch 259/2000\n",
      "2489/2489 [==============================] - 1s 455us/step - loss: 1722.4217\n",
      "Epoch 260/2000\n",
      "2489/2489 [==============================] - 1s 461us/step - loss: 3821.5357\n",
      "Epoch 261/2000\n",
      "2489/2489 [==============================] - 1s 451us/step - loss: 2175.4266\n",
      "Epoch 262/2000\n",
      "2489/2489 [==============================] - 1s 451us/step - loss: 518.3330\n",
      "Epoch 263/2000\n",
      "2489/2489 [==============================] - 1s 449us/step - loss: 1428.5459\n",
      "Epoch 264/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2489/2489 [==============================] - 1s 448us/step - loss: 4034.9570\n",
      "Epoch 265/2000\n",
      "2489/2489 [==============================] - 1s 449us/step - loss: 3870.2805\n",
      "Epoch 266/2000\n",
      "2489/2489 [==============================] - 1s 444us/step - loss: 11668.7524\n",
      "Epoch 267/2000\n",
      "2489/2489 [==============================] - 1s 445us/step - loss: 4540.9399\n",
      "Epoch 268/2000\n",
      "2489/2489 [==============================] - 1s 450us/step - loss: 4098.6426\n",
      "Epoch 269/2000\n",
      "2489/2489 [==============================] - 1s 454us/step - loss: 699.5915\n",
      "Epoch 270/2000\n",
      "2489/2489 [==============================] - 1s 444us/step - loss: 8405.7638\n",
      "Epoch 271/2000\n",
      "2489/2489 [==============================] - 1s 452us/step - loss: 7293.9103\n",
      "Epoch 272/2000\n",
      "2489/2489 [==============================] - 1s 444us/step - loss: 4998.9687\n",
      "Epoch 273/2000\n",
      "2489/2489 [==============================] - 1s 451us/step - loss: 180.2286\n",
      "Epoch 274/2000\n",
      "2489/2489 [==============================] - 1s 449us/step - loss: 2660.7679\n",
      "Epoch 275/2000\n",
      "2489/2489 [==============================] - 1s 444us/step - loss: 16865.5397\n",
      "Epoch 276/2000\n",
      "2489/2489 [==============================] - 1s 446us/step - loss: 3859.9622\n",
      "Epoch 277/2000\n",
      "2489/2489 [==============================] - 1s 444us/step - loss: 1126.7443\n",
      "Epoch 278/2000\n",
      "2489/2489 [==============================] - 1s 446us/step - loss: 143.4590\n",
      "Epoch 279/2000\n",
      "2489/2489 [==============================] - 1s 447us/step - loss: 8022.3951\n",
      "Epoch 280/2000\n",
      "2489/2489 [==============================] - 1s 449us/step - loss: 3640.4480\n",
      "Epoch 281/2000\n",
      "2489/2489 [==============================] - 1s 449us/step - loss: 8545.1429\n",
      "Epoch 282/2000\n",
      "2489/2489 [==============================] - 1s 450us/step - loss: 3191.7578\n",
      "Epoch 283/2000\n",
      "2489/2489 [==============================] - 1s 460us/step - loss: 2514.2914\n",
      "Epoch 284/2000\n",
      "2489/2489 [==============================] - 1s 446us/step - loss: 10551.6569\n",
      "Epoch 285/2000\n",
      "2489/2489 [==============================] - 1s 458us/step - loss: 26444.2007\n",
      "Epoch 286/2000\n",
      "2489/2489 [==============================] - 1s 439us/step - loss: 811.1904\n",
      "Epoch 287/2000\n",
      "2489/2489 [==============================] - 1s 445us/step - loss: 2283.2090\n",
      "Epoch 288/2000\n",
      "2489/2489 [==============================] - 1s 453us/step - loss: 25097.1287\n",
      "Epoch 289/2000\n",
      "2489/2489 [==============================] - 1s 446us/step - loss: 3478.1582\n",
      "Epoch 290/2000\n",
      "2489/2489 [==============================] - 1s 445us/step - loss: 1051.8170\n",
      "Epoch 291/2000\n",
      "2489/2489 [==============================] - 1s 446us/step - loss: 2203.9541\n",
      "Epoch 292/2000\n",
      "2489/2489 [==============================] - 1s 448us/step - loss: 8086.2160\n",
      "Epoch 293/2000\n",
      "2489/2489 [==============================] - 1s 448us/step - loss: 894.0673\n",
      "Epoch 294/2000\n",
      "2489/2489 [==============================] - 1s 457us/step - loss: 8179.6798\n",
      "Epoch 295/2000\n",
      "2489/2489 [==============================] - 1s 446us/step - loss: 7700.1044\n",
      "Epoch 296/2000\n",
      "2489/2489 [==============================] - 1s 456us/step - loss: 771.3617\n",
      "Epoch 297/2000\n",
      "2489/2489 [==============================] - 1s 443us/step - loss: 2835.7233\n",
      "Epoch 298/2000\n",
      "2489/2489 [==============================] - 1s 450us/step - loss: 1111.8281\n",
      "Epoch 299/2000\n",
      "2489/2489 [==============================] - 1s 447us/step - loss: 4907.1638\n",
      "Epoch 300/2000\n",
      "2489/2489 [==============================] - 1s 446us/step - loss: 2691.9034\n",
      "Epoch 301/2000\n",
      "2489/2489 [==============================] - 1s 451us/step - loss: 2340.7267\n",
      "Epoch 302/2000\n",
      "2489/2489 [==============================] - 1s 447us/step - loss: 8337.8358\n",
      "Epoch 303/2000\n",
      "2489/2489 [==============================] - 1s 450us/step - loss: 6015.4425\n",
      "Epoch 304/2000\n",
      "2489/2489 [==============================] - 1s 448us/step - loss: 4079.2764\n",
      "Epoch 305/2000\n",
      "2372/2489 [===========================>..] - ETA: 0s - loss: 547.5404"
     ]
    }
   ],
   "source": [
    "# 메인윈도우 생성 : 타이틀 - Multi Variable Matrix Linear Regression\n",
    "main = Tk()\n",
    "main.title(\"Multi Variable Matrix Linear Regression\")\n",
    "main.geometry()\n",
    "\n",
    "# 라벨생성 - 라벨 텍스트 : Multi Variable Matrix Linear Regression\n",
    "# 굴림체, 18포인트, row=0, column=0, columnspan=6\n",
    "label = Label(main, text='Multi Variable Matrix Linear Regression')\n",
    "label.config(font=(\"굴림\", 18))\n",
    "label.grid(row=0, column=0, columnspan=6)\n",
    "\n",
    "# 콤보상자 생성 - 콤보상자 내 item : gender_list, 위치( 1(row), 0(column) )\n",
    "t_gVal = StringVar(value=gender_list[0]) # 연결된 변수 값 - 최초표시값을 저장\n",
    "t_gCbbox = ttk.Combobox(main, textvariable=t_gVal)\n",
    "t_gCbbox['values'] = gender_list  # 전체 보유 item설정\n",
    "t_gCbbox.config(state='readonly')\n",
    "t_gCbbox.grid(row=1, column=1)\n",
    "\n",
    "# 라벨생성 텍스트 : Gender # row=1, column=0\n",
    "t_gLabel = Label(main, text='Gender : ')\n",
    "t_gLabel.grid(row=1, column=0)\n",
    "\n",
    "# 스핀박스 생성\n",
    "# 초기값 45, 시작값 18, 끝 값 84, 증가량 1, 오른쪽정렬, 스핀박스버튼 외 수정불가\n",
    "# row=1, column=3\n",
    "# 라벨 생성 # 텍스트 Age : row=1, column=4\n",
    "t_aVal = IntVar(value=45)\n",
    "t_aSpbox = Spinbox(main, textvariable=t_aVal, from_=18, to=84, increment=1, \\\n",
    "                                                                       justify=RIGHT)\n",
    "t_aSpbox.config(state='readonly')\n",
    "t_aSpbox.grid(row=1, column=3)\n",
    "t_aLabel=Label(main, text='Age : ')\n",
    "t_aLabel.grid(row=1, column=4)\n",
    "\n",
    "# 스핀박스2 생성\n",
    "# 초기값 500, 시작값 0, 끝 값 1500, 증가량 1, 오른쪽 정렬, 스핀박스버튼 외 수정불가\n",
    "# row=1, column=5\n",
    "# 라벨 생성 # 텍스트 Pace : row=1, column=4\n",
    "t_pVal = IntVar(value=500)\n",
    "t_pSpbox = Spinbox(main, textvariable=t_pVal, from_=0, to=1500, increment=1, \\\n",
    "                                                                       justify=RIGHT)\n",
    "t_pSpbox.config(state='readonly')\n",
    "t_pSpbox.grid(row=1, column=5)\n",
    "t_pLabel=Label(main, text='Pace : ')\n",
    "t_pLabel.grid(row=1, column=4)\n",
    "\n",
    "# 스핀박스3 생성\n",
    "# 초기값 2000, 시작값 0, 끝 값 100000, 증가량 1000, 오른쪽 정렬,\n",
    "# row=2, column=1\n",
    "# 라벨생성 #텍스트 Number of train : row=2, column=0\n",
    "t_tVal = IntVar(value=2000)\n",
    "t_tSpbox = Spinbox(main, textvariable=t_tVal, from_=0, to=100000, increment=1000, \\\n",
    "                                                                       justify=RIGHT)\n",
    "t_tSpbox.grid(row=2, column=1)\n",
    "t_tLabel=Label(main, text='Number of train : ')\n",
    "t_tLabel.grid(row=2, column=0)\n",
    "\n",
    "# 스핀박스4 생성\n",
    "# 초기값 0.000001, 시작값 0, 끝 값 1, 증가량 0.000001, 오른쪽 정렬,\n",
    "# row=2, column=3\n",
    "# 라벨 생성 # 텍스트 Learning rate : row=2, column=2\n",
    "t_rVal = DoubleVar(value=1e-6)\n",
    "t_rSpbox = Spinbox(main, textvariable=t_rVal, from_=0, to=1, increment=(1e-6), \\\n",
    "                                                                       justify=RIGHT)\n",
    "t_rSpbox.grid(row=2, column=3)\n",
    "t_rLabel=Label(main, text='Learning rate : ')\n",
    "t_rLabel.grid(row=2, column=2)\n",
    "\n",
    "# 버튼 생성 \n",
    "btn1 = Button(main, text=\"Histogram\", height=2,command=lambda:histogram())\n",
    "btn1.grid(row=2, column=4, columnspan=1, sticky=(W,E))\n",
    "btn2 = Button(main, text=\"Prediction\", height=2,command=lambda:learning())\n",
    "btn2.grid(row=2, column=5, columnspan=1, sticky=(W,E))\n",
    "\n",
    "# 차트 캔버스\n",
    "grad_canvas = FigureCanvasTkAgg(grad_fig, main)\n",
    "grad_canvas.get_tk_widget().grid(row=3, column=0, columnspan=6)\n",
    "\n",
    "# Scrolled Text\n",
    "log_ScrolledText = tkst.ScrolledText(main,height=15)\n",
    "log_ScrolledText.grid(row=4, column=0, columnspan=6, sticky=(N, S, W, E))\n",
    "log_ScrolledText.configure(font='굴림')\n",
    "log_ScrolledText.tag_config('RESULT', foreground='blue', font=(\"굴림\", 12))\n",
    "log_ScrolledText.tag_config('HEADER', foreground='black', font=(\"굴림\", 14))\n",
    "log_ScrolledText.tag_config('TITLE', foreground='red', font=(\"굴림\", 18))\n",
    "\n",
    "# 어제 데이터 : [5,10,15,20,21.098,25,30,35]과 [321, 758, 1470, 2145 ... 5021]\n",
    "# 오늘 데이터세트를 입력하고 특정 연령, 성별의 최종기록 예측\n",
    "\n",
    "\n",
    "main.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문장토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플 텍스트\n",
    "text_sample = 'The Matrix is everywhere its all around us. here even in this room. \\\n",
    "You can see it out your window or on your television. You feel it when you go to work. \\\n",
    "or go to church or pay your taxes.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화 모듈 import\n",
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 5\n",
      "['The Matrix is everywhere its all around us.', 'here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work.', 'or go to church or pay your taxes.']\n"
     ]
    }
   ],
   "source": [
    "# sent_toknize : 문장 또는 문서 내에서 마침표 기준으로 토큰화하는 함수입니다\n",
    "# 결과 : 문장들의 리스트\n",
    "sentences = sent_tokenize(text=text_sample)\n",
    "print(type(sentences),len(sentences))\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 9\n",
      "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "# 문장에서 단어단위로 공백기준 토큰화\n",
    "words = word_tokenize(sentences[0])\n",
    "print(type(words), len(words))\n",
    "print(words)\n",
    "# 공백으로 구분된 단어토큰화의 특이점 : ','도 하나의 단어로 인식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 여러 문장이 있는 문단(text_sample)에 대한 단어 토큰화 함수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 5\n",
      "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', '.'], ['here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', '.'], ['or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_text(text):\n",
    "    # 문장(마침표 기준)별로 분리 : sent_tokenize를 사용하여 리스트로 분리\n",
    "    sentences = sent_tokenize(text)\n",
    "    # 만들어진 리스트를 반복실행문에 적용, 결과를 별도의 리스트(word_tokens)에 저장\n",
    "    \n",
    "    # word_tokens = []\n",
    "    # for sentences in sentences:\n",
    "    #    word_token = word_tokenize(sentence)\n",
    "    #    word_tokens.append(word_token)\n",
    "    #return word_tokens  # 2차원 리스트 리턴\n",
    "    \n",
    "    #한 줄로 표기\n",
    "    word_tokens = [ word_tokenize(sentence) for sentence in sentences ]\n",
    "    return word_tokens  # 2차원 리스트 리턴\n",
    "    # ** a = [i for i in range(10)] 0~9의 숫자를 리스트로 만들어서 a변수에 저장\n",
    "word_tokens = tokenize_text(text_sample)\n",
    "print(type(word_tokens), len(word_tokens))\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "sentence = 'The Matrix is everywhere its all around us. here even in this room.'\n",
    "words = word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object ngrams at 0x000002F3D1510890>\n"
     ]
    }
   ],
   "source": [
    "all_ngrams = ngrams(words, 2)\n",
    "# all_ngrams = ngrams(words, 3)\n",
    "# 숫자 : 묶이는 단어의 수\n",
    "print(all_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'Matrix'), ('Matrix', 'is'), ('is', 'everywhere'), ('everywhere', 'its'), ('its', 'all'), ('all', 'around'), ('around', 'us'), ('us', '.'), ('.', 'here'), ('here', 'even'), ('even', 'in'), ('in', 'this'), ('this', 'room'), ('room', '.')]\n"
     ]
    }
   ],
   "source": [
    "ngrams = [ngram for ngram in all_ngrams]\n",
    "print(ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\204\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 stop words 개수: 179\n"
     ]
    }
   ],
   "source": [
    "print('영어 stop words 개수:', len(nltk.corpus.stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', '.'], ['here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', '.'], ['or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['matrix', 'everywhere', 'around', 'us', '.'], ['even', 'room', '.'], ['see', 'window', 'television', '.'], ['feel', 'go', 'work', '.'], ['go', 'church', 'pay', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "all_tokens = []\n",
    "# word_tokens 리스트에서 stopwords를 제거하기 위해 반복명령문 실행\n",
    "for sentence in word_tokens:\n",
    "    # 각 sentence에서 stopwords를 제외한 단어들이 들어갈 빈 리스트 생성\n",
    "    filtered_words = []\n",
    "    # sentence 안의 단어들을 이용한 반복실행\n",
    "    for word in sentence:\n",
    "        # 소문자 변환\n",
    "        word = word.lower()\n",
    "        # tokenize 된 개별 word가 stopwords들의 단어에 포함되지 않으면 word_tokens에 추가\n",
    "        if word not in stopwords:\n",
    "            filtered_words.append(word)\n",
    "    # 작업 종료한 sentence 내의 단어가 모여잇는 리스트를 최종 리스트에 추가\n",
    "    all_tokens.append(filtered_words)\n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming(단어의 원형 찾기)과 Lemmatization(단어의 변형 찾기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n",
      "amus amus amus\n",
      "happy happiest\n",
      "fant fanciest\n"
     ]
    }
   ],
   "source": [
    "# Stemming\n",
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "print(stemmer.stem('working'), stemmer.stem('works'), stemmer.stem('worked'))\n",
    "print(stemmer.stem('amusing'), stemmer.stem('amuses'), stemmer.stem('amused'))\n",
    "print(stemmer.stem('happier'), stemmer.stem('happiest'))\n",
    "print(stemmer.stem('fancier'), stemmer.stem('fanciest'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amuse amuse amuse\n",
      "happy happy\n",
      "fancy fancy\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# a: 형용사  v: 동사\n",
    "lemma = WordNetLemmatizer()\n",
    "print(lemma.lemmatize('amusing', 'v'), lemma.lemmatize('amuses','v'), \\\n",
    "                                          lemma.lemmatize('amused','v'))\n",
    "print(lemma.lemmatize('happier','a'), lemma.lemmatize('happiest','a'))\n",
    "print(lemma.lemmatize('fancier','a'), lemma.lemmatize('fanciest','a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words - BOW\n",
    "### 사이킷런 CountVectorizer 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sample_01 = 'The Matrix is everywhere its all around us. here even in this room. \\\n",
    "You can see it out your window or on your television. You feel it when you go to work. \\\n",
    "or go to church or pay your taxes.'\n",
    "text_sample_02 = 'You take the blue pill and the story ends. You wake in your bed and \\\n",
    "you believe whatever you want to believe. You take the red pill and you stay in Wonderland\\\n",
    "and I show you how deep the rabbit-hole goes.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=[]\n",
    "text.append(text_sample_01);\n",
    "text.append(text_sample_02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Matrix is everywhere its all around us. here even in this room. You can see it out your window or on your television. You feel it when you go to work. or go to church or pay your taxes.', 'You take the blue pill and the story ends. You wake in your bed and you believe whatever you want to believe. You take the red pill and you stay in Wonderlandand I show you how deep the rabbit-hole goes.'] \n",
      " 2\n"
     ]
    }
   ],
   "source": [
    "print(text, \"\\n\", len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CountVectorizer 객체 생성 후 fit()\n",
    "* transform()으로 텍스트에 대한 feature vectorization 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Count Vectorizer 객체 생성 및 fit\n",
    "cnt_vect = CountVectorizer()\n",
    "# fit : 문장안에서 단어를 찾아내어 cnt_vect에 저장\n",
    "cnt_vect.fit(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform()\n",
    "# 2행 52열짜리 행렬로 변환\n",
    "ftr_vect = cnt_vect.transform(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**피처 벡터화 후 데이터 유형 및 여러 속성 확인**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'> (2, 51)\n"
     ]
    }
   ],
   "source": [
    "print(type(ftr_vect), ftr_vect.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 12)\t1\n",
      "  (0, 13)\t2\n",
      "  (0, 15)\t1\n",
      "  (0, 18)\t1\n",
      "  (0, 19)\t1\n",
      "  (0, 20)\t2\n",
      "  (0, 21)\t1\n",
      "  (0, 22)\t1\n",
      "  (0, 23)\t1\n",
      "  (0, 24)\t3\n",
      "  (0, 25)\t1\n",
      "  (0, 26)\t1\n",
      "  (0, 30)\t1\n",
      "  (0, 31)\t1\n",
      "  (0, 36)\t1\n",
      "  (0, 37)\t1\n",
      "  (0, 38)\t1\n",
      "  (0, 39)\t1\n",
      "  (0, 40)\t2\n",
      "  :\t:\n",
      "  (1, 1)\t3\n",
      "  (1, 3)\t1\n",
      "  (1, 4)\t2\n",
      "  (1, 5)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 14)\t1\n",
      "  (1, 16)\t1\n",
      "  (1, 17)\t1\n",
      "  (1, 18)\t2\n",
      "  (1, 27)\t2\n",
      "  (1, 28)\t1\n",
      "  (1, 29)\t1\n",
      "  (1, 32)\t1\n",
      "  (1, 33)\t1\n",
      "  (1, 34)\t1\n",
      "  (1, 35)\t2\n",
      "  (1, 38)\t4\n",
      "  (1, 40)\t1\n",
      "  (1, 42)\t1\n",
      "  (1, 43)\t1\n",
      "  (1, 44)\t1\n",
      "  (1, 47)\t1\n",
      "  (1, 49)\t7\n",
      "  (1, 50)\t1\n"
     ]
    }
   ],
   "source": [
    "print(ftr_vect)\n",
    "# (a, b)\n",
    "# a : 1번 문단 혹은 2번 문단\n",
    "# b : 해당 단어의 고유번호\n",
    "# ftr_vect.shape : 해당 단어가 나온 회수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 38, 'matrix': 22, 'is': 19, 'everywhere': 11, 'its': 21, 'all': 0, 'around': 2, 'us': 41, 'here': 15, 'even': 10, 'in': 18, 'this': 39, 'room': 30, 'you': 49, 'can': 6, 'see': 31, 'it': 20, 'out': 25, 'your': 50, 'window': 46, 'or': 24, 'on': 23, 'television': 37, 'feel': 12, 'when': 45, 'go': 13, 'to': 40, 'work': 48, 'church': 7, 'pay': 26, 'taxes': 36, 'take': 35, 'blue': 5, 'pill': 27, 'and': 1, 'story': 34, 'ends': 9, 'wake': 42, 'bed': 3, 'believe': 4, 'whatever': 44, 'want': 43, 'red': 29, 'stay': 33, 'wonderlandand': 47, 'show': 32, 'how': 17, 'deep': 8, 'rabbit': 28, 'hole': 16, 'goes': 14}\n"
     ]
    }
   ],
   "source": [
    "print(cnt_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**stopwords 제거 & 출현빈도수에 의한 word 필터링 CountVectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 10)\n",
      "{'room': 2, 'window': 9, 'television': 6, 'taxes': 5, 'pill': 1, 'story': 4, 'wake': 7, 'believe': 0, 'want': 8, 'stay': 3}\n"
     ]
    }
   ],
   "source": [
    "# stopwords 제거 & 출현빈도 수에 의한 \n",
    "cnt_vect = CountVectorizer( max_features = 10, stop_words='english')\n",
    "cnt_vect.fit(text)\n",
    "ftr_vect = cnt_vect.transform(text)\n",
    "print(ftr_vect.shape)\n",
    "print(cnt_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ngram_range를 적용한 CountVectorizer 확인**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 124)\n",
      "{'the': 81, 'matrix': 48, 'is': 41, 'everywhere': 24, 'its': 46, 'all': 0, 'around': 5, 'us': 93, 'here': 31, 'even': 22, 'in': 37, 'this': 87, 'room': 66, 'you': 109, 'can': 14, 'see': 68, 'it': 43, 'out': 56, 'your': 119, 'window': 103, 'or': 52, 'on': 50, 'television': 79, 'feel': 26, 'when': 101, 'go': 28, 'to': 89, 'work': 107, 'church': 16, 'pay': 58, 'taxes': 78, 'the matrix': 83, 'matrix is': 49, 'is everywhere': 42, 'everywhere its': 25, 'its all': 47, 'all around': 1, 'around us': 6, 'us here': 94, 'here even': 32, 'even in': 23, 'in this': 38, 'this room': 88, 'room you': 67, 'you can': 111, 'can see': 15, 'see it': 69, 'it out': 44, 'out your': 57, 'your window': 123, 'window or': 104, 'or on': 54, 'on your': 51, 'your television': 122, 'television you': 80, 'you feel': 112, 'feel it': 27, 'it when': 45, 'when you': 102, 'you go': 113, 'go to': 29, 'to work': 92, 'work or': 108, 'or go': 53, 'to church': 91, 'church or': 17, 'or pay': 55, 'pay your': 59, 'your taxes': 121, 'take': 76, 'blue': 12, 'pill': 60, 'and': 2, 'story': 74, 'ends': 20, 'wake': 95, 'bed': 7, 'believe': 9, 'whatever': 99, 'want': 97, 'red': 64, 'stay': 72, 'wonderlandand': 105, 'show': 70, 'how': 35, 'deep': 18, 'rabbit': 62, 'hole': 33, 'goes': 30, 'you take': 116, 'take the': 77, 'the blue': 82, 'blue pill': 13, 'pill and': 61, 'and the': 3, 'the story': 86, 'story ends': 75, 'ends you': 21, 'you wake': 117, 'wake in': 96, 'in your': 40, 'your bed': 120, 'bed and': 8, 'and you': 4, 'you believe': 110, 'believe whatever': 10, 'whatever you': 100, 'you want': 118, 'want to': 98, 'to believe': 90, 'believe you': 11, 'the red': 85, 'red pill': 65, 'you stay': 115, 'stay in': 73, 'in wonderlandand': 39, 'wonderlandand show': 106, 'show you': 71, 'you how': 114, 'how deep': 36, 'deep the': 19, 'the rabbit': 84, 'rabbit hole': 63, 'hole goes': 34}\n"
     ]
    }
   ],
   "source": [
    "# 한 단어 & 두 단어씩 묶인 gram이 적용되어 피쳐 구성 : 124개의 피쳐 생성\n",
    "cnt_vect = CountVectorizer(ngram_range=(1,2))\n",
    "cnt_vect.fit(text)\n",
    "ftr_vect = cnt_vect.transform(text)\n",
    "print(ftr_vect.shape)\n",
    "print(cnt_vect.vocabulary_)\n",
    "# print( cnt_vect )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 희소 행렬(Sparse Matrix)\n",
    "* 대부분의 값이 0으로 채워진 행렬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* COO(Coordinate : 좌표) 형식\n",
    "* CSR(Compressed Sparse Row) 형식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. COO 형식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 0 1]\n",
      " [0 2 0]]\n"
     ]
    }
   ],
   "source": [
    "# COO 형식 실습을 위한 행렬 생성\n",
    "import numpy as np\n",
    "# (0,0)-3  (0,2)-1  (1,1)-2\n",
    "dense = np.array( [ [3, 0, 1],\n",
    "                    [0, 2, 0] ] )\n",
    "print(dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "  (0, 0)\t3\n",
      "  (0, 2)\t1\n",
      "  (1, 1)\t2\n",
      "<class 'numpy.ndarray'> \n",
      " [[3 0 1]\n",
      " [0 2 0]]\n"
     ]
    }
   ],
   "source": [
    "# 희소행렬 사용을 위한 모듈 import\n",
    "from scipy import sparse\n",
    "# 첫 번째, 행렬에서 0이 아닌 데이터만 추출합니다\n",
    "data = np.array([3, 1, 2])\n",
    "# 두 번째, 추출된 데이터들이 위치했던 좌표값을 array에 만들고 저장합니다(행, 열 별도)\n",
    "row_pos = np.array([0,0,1])\n",
    "col_pos = np.array([0,2,1])\n",
    "# 세 번째, coo_matrix를 이용하여 coo 형식의 희소행렬로 생성'\n",
    "sparse_coo = sparse.coo_matrix((data, (row_pos, col_pos)))\n",
    "# 출력\n",
    "print(type(sparse_coo))  # 데이터 타입\n",
    "print(sparse_coo)  # 저장값들의 출력\n",
    "dense01 = sparse_coo.toarray()  # 원본행렬 추출\n",
    "print(type(dense01), \"\\n\", dense01)  # 원본행렬 데이터타입과 내용 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. CSR 형식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실습용 행렬 생성\n",
    "dense2 = np.array([[0,0,1,0,0,5],\n",
    "                   [1,4,0,3,2,5],\n",
    "                   [0,6,0,3,0,0],\n",
    "                   [2,0,0,0,0,0],\n",
    "                   [0,0,0,7,0,8],\n",
    "                   [1,0,0,0,0,0] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 첫 번째, 0이 아닌 데이터만 추출합니다\n",
    "data2 = np.array([1,5,1,4,3,2,5,6,3,2,7,8,1])\n",
    "# 두 번째, 행 위치와 열 위치를 각각 array로 생성\n",
    "row_pos = np.array([0,0,1,1,1,1,1,2,2,3,4,4,5])\n",
    "col_pos = np.array([2,5,0,1,3,4,5,1,3,0,3,5,0])\n",
    "# 1, 2과정은 COO와 동일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COO 형식\n",
    "sparse_coo = sparse.coo_matrix((data2, (row_pos, col_pos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력확인\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n",
      "CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력확인\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# 세 번째\n",
    "# row_pos = np.array([0,0,1,1,1,1,1,2,2,3,4,4,5]) 의 배열 인덱스를 확인\n",
    "# index : 0,1,2,3,4,5,6,,8,9,10,11,12\n",
    "# 숫자가 변하는 곳의 인덱스만 따로 모아 배열로 만듬 - 13은 마지막 종료\n",
    "row_pos_ind = np.array([0,2,7,9,10,12,13])\n",
    "# CSR 형식으로 변환\n",
    "sparse_csr = sparse.csr_matrix((data2, col_pos, row_pos_ind))\n",
    "print('COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력확인')\n",
    "print(sparse_coo.toarray())\n",
    "print('CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력확인')\n",
    "print(sparse_csr.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
